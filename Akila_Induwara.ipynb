{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOb8CGX1IsCYPg8xYafsoPA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akilaIduwara/akilaiduwara/blob/main/Akila_Induwara.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Underground Water Detect Algorithms"
      ],
      "metadata": {
        "id": "TS21JrxGPY4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "swDbSwMfPidp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable Mixed Precision (if using GPU)\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "CnxssEe5QSze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load and Preprocess Data Efficiently\n",
        "try:\n",
        "    df = pd.read_csv('/content/JWD_L2B_DD_ERTmodel.csv', skiprows=1, header=0)\n",
        "    df.columns = df.columns.str.strip()\n",
        "    print(\"Columns in the CSV file:\", df.columns.tolist())\n",
        "\n",
        "    if 'Log10Res' not in df.columns:\n",
        "        raise KeyError(\"Column 'Log10Res' not found in the CSV file. Please check the column names.\")\n",
        "    # Filter invalid values\n",
        "    df = df[df['Log10Res'] != -9999]\n",
        "    df['Resistivity'] = 10 ** df['Log10Res']\n",
        "    df['is_water'] = np.where((df['Resistivity'] >= 20) & (df['Resistivity'] <= 100), 1, 0)\n",
        "\n",
        "    features = df[['Dist', 'X_NAD83UTMz16N', 'Y_NAD83UTMz16N', 'Elev']]\n",
        "    target = df['is_water']\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error during data loading or preprocessing:\", e)\n",
        "    exit()\n",
        "\n"
      ],
      "metadata": {
        "id": "wfDjis1MQcYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train ANN Deep Learning Model with Large Data Handling\n",
        "try:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, target, test_size=0.2, random_state=42\n",
        "    )\n",
        " # Normalize features\n",
        "    mean = X_train.mean()\n",
        "    std = X_train.std()\n",
        "    X_train = (X_train - mean) / std\n",
        "    X_test = (X_test - mean) / std\n",
        "\n",
        " # Convert to TensorFlow datasets for efficiency\n",
        "    batch_size = 64\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values)).shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        " # Define ANN model\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        keras.layers.Dropout(0.3),  # Prevents overfitting\n",
        "        keras.layers.Dense(128, activation='relu'),\n",
        "        keras.layers.Dropout(0.3),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        " # Early stopping callback to prevent overtraining\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        " # Train the model\n",
        "    model.fit(train_dataset, epochs=30, validation_data=test_dataset, callbacks=[early_stopping])\n",
        "\n",
        "\n",
        " # Evaluate model\n",
        "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error during model training or evaluation:\", e)\n",
        "    exit()\n",
        "\n"
      ],
      "metadata": {
        "id": "5m16N96oQ47l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Identify Water Resources and Display Results Efficiently\n",
        "try:\n",
        "    # Normalize the full dataset\n",
        "    df_normalized = (features - mean) / std\n",
        "    dataset_full = tf.data.Dataset.from_tensor_slices(df_normalized.values).batch(batch_size)\n",
        "\n",
        "    # Predict water locations using optimized batch inference\n",
        "    predictions = (model.predict(dataset_full) > 0.5).astype(int)\n",
        "    df['predicted_water'] = predictions\n",
        "\n",
        "\n",
        "    # Filter water locations\n",
        "    water_locations = df[df['predicted_water'] == 1]\n",
        "\n",
        "    results = water_locations[['X_NAD83UTMz16N', 'Y_NAD83UTMz16N', 'Elev']].rename(columns={'Elev': 'Depth'})\n",
        "    results['Depth'] = np.abs(results['Depth'])\n",
        "\n",
        "    print(\"\\nIdentified Water Resources:\")\n",
        "    print(results.head(10).to_string(index=False, justify='left'))\n",
        "\n",
        "    results.to_csv(\"identified_water_resources.csv\", index=False)\n",
        "    print(\"Results saved to identified_water_resources.csv\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error during prediction or result display:\", e)\n",
        "    exit()\n"
      ],
      "metadata": {
        "id": "rQ10bS20SAnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Underground Gem Detect Algorithms\n"
      ],
      "metadata": {
        "id": "ZffMNqIbSpTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "u8Rj8joLS9ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable Mixed Precision (if using GPU)\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "\n",
        "# Step 2: Load and Preprocess Data Efficiently\n",
        "\n",
        "try:\n",
        "    # Load datasets\n",
        "    radiometrics = pd.read_csv('radiometrics.csv')\n",
        "    magnetics = pd.read_csv('magnetics.csv')\n",
        "    gravity = pd.read_csv('gravity.csv')\n",
        "    resistivity = pd.read_csv('resistivity.csv')\n",
        "\n",
        "    # Merge datasets on common coordinates (X, Y)\n",
        "    data = pd.merge(radiometrics, magnetics, on=['X', 'Y'])\n",
        "    data = pd.merge(data, gravity, on=['X', 'Y'])\n",
        "    data = pd.merge(data, resistivity, on=['X', 'Y'])\n",
        "\n",
        "    # Feature engineering\n",
        "    data['K_Th_ratio'] = data['Potassium_NASVD_processed'] / data['Thorium_NASVD_processed']\n",
        "    data['U_Th_ratio'] = data['Uranium_NASVD_processed'] / data['Thorium_NASVD_processed']\n",
        "\n",
        "\n",
        "    # Define features and target\n",
        "    features = ['K_Th_ratio', 'U_Th_ratio', 'Processed_magnetics', 'Gravity_disturbance_Processed', 'Log10Res']\n",
        "    target = 'Gold_Label'  # Replace with actual labels if available\n",
        "\n",
        "    # Filter invalid values\n",
        "    data = data[data['Log10Res'] != -9999]\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = MinMaxScaler()\n",
        "    data_normalized = scaler.fit_transform(data[features])\n",
        "\n",
        "\n",
        "    # Add target column to normalized data\n",
        "    data_normalized = pd.DataFrame(data_normalized, columns=features)\n",
        "    data_normalized[target] = data[target]\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error during data loading or preprocessing:\", e)\n",
        "    exit()\n",
        "\n"
      ],
      "metadata": {
        "id": "GGoHHWyyUDJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train ANN Deep Learning Model with Large Data Handling\n",
        "\n",
        "try:\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data_normalized[features], data_normalized[target], test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Convert to TensorFlow datasets for efficiency\n",
        "    batch_size = 64\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values)).shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Define ANN model\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        keras.layers.Dropout(0.3),  # Prevents overfitting\n",
        "        keras.layers.Dense(128, activation='relu'),\n",
        "        keras.layers.Dropout(0.3),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Early stopping callback to prevent overtraining\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "cWrbTBCJVYSX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}